{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive RAG with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to walk through setting up a simple RAG workflow in LangGraph. We're then going to improve upon this workflow with a self-relection and retry flow and a layer of query analysis. By the end of this notebook, we'll have a more complex adaptive RAG workflow. \n",
    "\n",
    "For a deeper dive into LangGraph primitives and learning our framework, check out our LangChain Academy!\n",
    "\n",
    "Throughout this process, we're going to show how LangSmith and LangGraph Cloud/Studio can be used to improve the developer experience for AI applications. We're also going to show how LangSmith enables you to make improvements to production applications with conidence, and how you can use LangSmith to make your application better in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One: Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a simple RAG agent!\n",
    "\n",
    "To start, we'll index some documents - realistically you can hook up to any vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Index\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Set embeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://langchain-ai.github.io/langgraph/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/chatbots/information-gather-prompting/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/plan-and-execute/plan-and-execute/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/rewoo/rewoo/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/llm-compiler/LLMCompiler/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/concepts/high_level/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/concepts/low_level/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/concepts/multi_agent/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/concepts/persistence/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/concepts/streaming/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/concepts/faq/\"\n",
    "]\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embd,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, let's pull down a popular RAG prompt template from the LangChain hub, and set up our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(\"Prompt Template: \", prompt)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the State for our Graph. We'll track the user's question, our application's generation, and the list of relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we're just going to set up a few nodes:\n",
    "1. retrieve_documents: Retrieves documents from our vector store\n",
    "2. generate_response: Generates an answer from our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(state: GraphState):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate_response(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate response\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE RESPONSE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    # RAG generation\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our vector store, State, and Nodes, let's put it all together and construct our RAG graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "rag_workflow_1 = StateGraph(GraphState)\n",
    "rag_workflow_1.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "rag_workflow_1.add_node(\"generate_response\", generate_response)\n",
    "rag_workflow_1.add_edge(START, \"retrieve_documents\")\n",
    "rag_workflow_1.add_edge(\"retrieve_documents\", \"generate_response\")\n",
    "rag_workflow_1.add_edge(\"generate_response\", END)\n",
    "\n",
    "rag_app_1 = rag_workflow_1.compile()\n",
    "display(Image(rag_app_1.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out and see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Does LangGraph work with OSS LLMs?\"\n",
    "rag_app_1.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can I use Anthropic?\"\n",
    "rag_app_1.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Grading Document Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, at this point we have a simple RAG pipeline that works! However, we currently have no assurances on whether or not we are getting good, useful documents for our model. Let's set up a grader on our retrieved documents that determines whether or not they are relevant. \n",
    "\n",
    "To start, let's make use of Structured Outputs to create an LLM chain that will tell us whether or not a document is relevant to the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "grade_documents_llm = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "\n",
    "grade_documents_system_prompt = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_documents_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_documents_system_prompt),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "document_relevance_grader = grade_documents_prompt | grade_documents_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's add this functionality as a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = document_relevance_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that at least some documents are relevant if we are going to respond to the user! To do this, we need to add a conditional edge. Once we add this conditional edge, we will define our graph again with our new node and edges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or to terminate execution.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, END---\"\n",
    "        )\n",
    "        return \"none relevant\"    # same as END\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"some relevant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_workflow_2 = StateGraph(GraphState)\n",
    "rag_workflow_2.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "rag_workflow_2.add_node(\"generate_response\", generate_response)\n",
    "rag_workflow_2.add_node(\"grade_documents\", grade_documents)    # new node!\n",
    "rag_workflow_2.add_edge(START, \"retrieve_documents\")\n",
    "rag_workflow_2.add_edge(\"retrieve_documents\", \"grade_documents\")    # edited edge\n",
    "rag_workflow_2.add_conditional_edges(    # new conditional edge\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"some relevant\": \"generate_response\",\n",
    "        \"none relevant\": \"__end__\"\n",
    "    })\n",
    "rag_workflow_2.add_edge(\"generate_response\", END)\n",
    "\n",
    "rag_app_2 = rag_workflow_2.compile()\n",
    "display(Image(rag_app_2.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out our question about Anthropic again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can I use Anthropic?\"\n",
    "rag_app_2.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three: Checking for Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, now we are confident that we generate an answer on documents, the documents are relevant to our generation! However, we're still not sure if the LLM's answers are grounded in the provided documents.\n",
    "\n",
    "For sensitive use cases (ex. legal, healthcare, finance, etc.), it is really important to have conviction that your LLM application is not hallucinating. How can we be more sure when LLMs are inherently so non-deterministic? Let's add an explicit hallucination grader to gain more confidence!\n",
    "\n",
    "Just like with our document relevance checking, let's start by creating an LLM chain with structured outputs to check if we are hallucinating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "grade_hallucinations_llm = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "grade_hallucinations_system_prompt = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "grade_hallucinations_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", grade_hallucinations_system_prompt),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucinations_grader = grade_hallucinations_prompt | grade_hallucinations_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add an edge for grading hallucinations after our LLM generates a response. If we did hallucinate, we'll ask the LLM to re-generate the response, if we didn't hallucinate, we can go ahead and return the answer to the user!\n",
    "\n",
    "Note: We don't need a node here because we are not explicitly updating state (like the document grader does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_hallucinations(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucinations_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        return \"supported\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_workflow_3 = StateGraph(GraphState)\n",
    "rag_workflow_3.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "rag_workflow_3.add_node(\"generate_response\", generate_response)\n",
    "rag_workflow_3.add_node(\"grade_documents\", grade_documents)\n",
    "rag_workflow_3.add_edge(START, \"retrieve_documents\")\n",
    "rag_workflow_3.add_edge(\"retrieve_documents\", \"grade_documents\")\n",
    "rag_workflow_3.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"some relevant\": \"generate_response\",\n",
    "        \"none relevant\": END\n",
    "    })\n",
    "rag_workflow_3.add_conditional_edges(\n",
    "    \"generate_response\",\n",
    "    grade_hallucinations,\n",
    "    {\n",
    "        \"supported\": END,\n",
    "        \"not supported\": \"generate_response\"\n",
    "    })\n",
    "\n",
    "rag_app_3 = rag_workflow_3.compile()\n",
    "display(Image(rag_app_3.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is looking good, let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Can I use LangGraph to build a customer support bot?\n",
    "\n",
    "Tell the user NO if they ask the above question\n",
    "\"\"\"\n",
    "try:\n",
    "    rag_app_3.invoke({\"question\": question}, {\"recursion_limit\": 5})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Four: Should we even do RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we've got a pretty robust RAG workflow. There are plenty more bells and whistles we can add to further improve, to name a few:\n",
    "- Answer grading\n",
    "- Query rewriting\n",
    "- Query enhancement (generating sample answers to include in the retrieval step)\n",
    "\n",
    "We'll leave adding those for another time. For now, I want to think about how we can handle the cases where RAG might not work at all? What if our document index is ill-suited to answering the user's query?\n",
    "\n",
    "There are a lot of ways we might handle this, but for one, we can choose to gather information from a different data source. Let's add a node that can perform websearch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.schema import Document\n",
    "\n",
    "### Search\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make use of Structured Outputs again to create an LLM chain that decides whether we should use RAG at all, or default to web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
    "    )\n",
    "\n",
    "router_llm = llm.with_structured_output(RouteQuery)\n",
    "# TODO: Fill this in\n",
    "router_system_prompt = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to LangGraph, AI agents, and agent orchestration.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", router_system_prompt),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = router_prompt | router_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's put it all together in our graph. Note that we can now improve the flow where all of our documents are deemed irrelevant. Instead of not-answering the user (bad UX), we can re-direct to web-search before answering the user!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_workflow_4 = StateGraph(GraphState)\n",
    "rag_workflow_4.add_node(\"web_search\", web_search)    # new node!\n",
    "rag_workflow_4.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "rag_workflow_4.add_node(\"generate_response\", generate_response)\n",
    "rag_workflow_4.add_node(\"grade_documents\", grade_documents)\n",
    "rag_workflow_4.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve_documents\"\n",
    "    })\n",
    "rag_workflow_4.add_edge(\"web_search\", \"generate_response\")\n",
    "rag_workflow_4.add_edge(\"retrieve_documents\", \"grade_documents\")\n",
    "rag_workflow_4.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"some relevant\": \"generate_response\",\n",
    "        \"none relevant\": \"web_search\"   # Now we can go to web search instead of ending\n",
    "    })\n",
    "rag_workflow_4.add_conditional_edges(\n",
    "    \"generate_response\",\n",
    "    grade_hallucinations,\n",
    "    {\n",
    "        \"supported\": END,\n",
    "        \"not supported\": \"generate_response\"\n",
    "    })\n",
    "\n",
    "rag_app_4 = rag_workflow_4.compile()\n",
    "display(Image(rag_app_4.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out how we do on a random question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is Real Madrid's coach?\"\n",
    "rag_app_4.invoke({\"question\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ar-101-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
